import cv2
import mediapipe as mp
import numpy as np
from tensorflow.keras.models import load_model
from PIL import ImageFont, ImageDraw, Image
import os

# ê¸°ë³¸ ì„¤ì •
MODEL_PATH = "model/asl_model.h5"
LABELS_PATH = "model/labels.npy"
FONT_FILENAME = "GmarketSansTTFMedium.ttf"
DEFAULT_FONT_PATH = os.path.join(os.path.dirname(__file__), FONT_FILENAME)
FONT_SIZE = 60
TEXT_POSITION = (50, 30)
TEXT_COLOR_RGB = (0, 0, 255)
BOX_COLOR_BGR = (255, 0, 0)

# ëª¨ë¸ê³¼ ë¼ë²¨ ë¡œë”©
try:
    model = load_model(MODEL_PATH)
    labels = np.load(LABELS_PATH, allow_pickle=True)
except Exception as e:
    print(f"ëª¨ë¸ ë˜ëŠ” ë¼ë²¨ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit()

# Mediapipe ì„¸íŒ…
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# ì¹´ë©”ë¼
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("ì˜¤ë¥˜: ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# í°íŠ¸ ì„¤ì •
font_path_to_use = DEFAULT_FONT_PATH
if not os.path.exists(font_path_to_use):
    font_path_to_use_windows = "C:/Windows/Fonts/GmarketSansTTFMedium.ttf"
    if os.path.exists(font_path_to_use_windows):
        font_path_to_use = font_path_to_use_windows
    else:
        font_path_to_use = None

pil_font = None
if font_path_to_use:
    try:
        pil_font = ImageFont.truetype(font_path_to_use, FONT_SIZE)
    except:
        pil_font = None

print("ìˆ˜í™” ì¸ì‹ì„ ì‹œì‘í•˜ë ¤ë©´ SPACEë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ ESCë¥¼ ëˆ„ë¥´ì„¸ìš”.")

latest_char = ""

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("ì˜¤ë¥˜: ì´ë¯¸ì§€ ìº¡ì²˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        break

    image = cv2.flip(frame, 1)
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb_image)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    # ê¸€ì ì¶œë ¥ (ì§ì „ ì¸ì‹ ê²°ê³¼)
    if pil_font and latest_char:
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        draw.text(TEXT_POSITION, latest_char, font=pil_font, fill=TEXT_COLOR_RGB)
        image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
    elif latest_char:
        cv2.putText(image, latest_char, TEXT_POSITION, cv2.FONT_HERSHEY_SIMPLEX, 2, BOX_COLOR_BGR, 3)

    cv2.imshow("ìˆ˜í™” ì¸ì‹", image)

    key = cv2.waitKey(10) & 0xFF

    if key == 27:  # ESC
        break
    elif key == 32:  # SPACE
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                coords = []
                for lm in hand_landmarks.landmark:
                    coords.extend([lm.x, lm.y])

                if len(coords) == model.input_shape[1]:
                    coords_array = np.array(coords).reshape(1, -1)
                    prediction = model.predict(coords_array)
                    char_index = np.argmax(prediction)

                    if 0 <= char_index < len(labels):
                        latest_char = labels[char_index]
                        print(f"ğŸ”¤ ì¸ì‹ ê²°ê³¼: {latest_char}")
                    else:
                        latest_char = "ERR:IDX"
                        print("âš ï¸ ì˜ˆì¸¡ ê²°ê³¼ ì¸ë±ìŠ¤ ì˜¤ë¥˜")
                else:
                    latest_char = "ERR:DIM"
                    print(f"âš ï¸ ì¢Œí‘œ ìˆ˜ ë¶ˆì¼ì¹˜: {len(coords)} â†’ í•„ìš”: {model.input_shape[1]}")
        else:
            print("ğŸ–ï¸ ì†ì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

cap.release()
cv2.destroyAllWindows()
hands.close()

print("ì¸ì‹ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
