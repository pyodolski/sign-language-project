from flask import Flask, render_template, Response, jsonify
import cv2
import mediapipe as mp
import numpy as np
import time
import tensorflow as tf
import os
from deep_translator import GoogleTranslator
from gtts import gTTS
import subprocess
from jamo import combine_hangul_jamo

app = Flask(__name__)

# ==== ê²½ë¡œ ì„¤ì • ====
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, "model")

ASL_MODEL_PATH = os.path.join(MODEL_DIR, "asl_model.tflite")
ASL_LABELS_PATH = os.path.join(MODEL_DIR, "asl_labels.npy")

KSL_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model.tflite")
KSL_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels.npy")

# ==== ëª¨ë¸ ë¡œë”© ====
try:
    asl_interpreter = tf.lite.Interpreter(model_path=ASL_MODEL_PATH)
    asl_interpreter.allocate_tensors()
    asl_input_details = asl_interpreter.get_input_details()
    asl_output_details = asl_interpreter.get_output_details()
    labels_asl = np.load(ASL_LABELS_PATH, allow_pickle=True)

    ksl_interpreter = tf.lite.Interpreter(model_path=KSL_MODEL_PATH)
    ksl_interpreter.allocate_tensors()
    ksl_input_details = ksl_interpreter.get_input_details()
    ksl_output_details = ksl_interpreter.get_output_details()
    labels_ksl = np.load(KSL_LABELS_PATH, allow_pickle=True)

    print("âœ… ASL, KSL ëª¨ë¸ ë° ë¼ë²¨ ë¡œë”© ì„±ê³µ")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    exit()

# ==== Mediapipe ì„¤ì • ====
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# ==== ì¸ì‹ ê²°ê³¼ ì €ì¥ ====
recognized_string = {"asl": "", "ksl": ""}
latest_char = {"asl": "", "ksl": ""}

# ==== ê³µí†µ ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ====
def generate_frames(interpreter, input_details, output_details, labels, lang_key):
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))

    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨")
        return

    last_prediction_time = 0
    prediction_interval = 2  # seconds
    prev_idx = -1
    process_active = True
    last_switch_time = time.time()
    active_duration = 2
    inactive_duration = 2

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if len(frame.shape) == 2 or frame.shape[2] == 1:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)

            image = cv2.flip(frame, 1)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            current_time = time.time()

            if process_active and current_time - last_switch_time >= active_duration:
                process_active = False
                last_switch_time = current_time
                print("ğŸ›‘ Mediapipe ë¹„í™œì„±í™” (2ì´ˆ íœ´ì‹)")
            elif not process_active and current_time - last_switch_time >= inactive_duration:
                process_active = True
                last_switch_time = current_time
                print("âœ… Mediapipe í™œì„±í™” (2ì´ˆ ì‹¤í–‰)")

            if process_active:
                result = hands.process(rgb_image)

                if result.multi_hand_landmarks:
                    for hand_landmarks in result.multi_hand_landmarks:
                        mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                        if current_time - last_prediction_time >= prediction_interval:
                            coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                            input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                            interpreter.set_tensor(input_details[0]['index'], input_data)
                            interpreter.invoke()
                            prediction = interpreter.get_tensor(output_details[0]['index'])
                            idx = np.argmax(prediction)

                            if 0 <= idx < len(labels):
                                latest_char[lang_key] = labels[idx]
                            else:
                                latest_char[lang_key] = "ERR:IDX"

                            prev_idx = idx
                            last_prediction_time = current_time

            display_text = f"í˜„ì¬: {latest_char[lang_key]} | ëˆ„ì : {recognized_string[lang_key]}"
            cv2.putText(image, display_text, (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            ret, buffer = cv2.imencode('.jpg', image)
            frame = buffer.tobytes()

            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    except GeneratorExit:
        print("ğŸ›‘ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ê°ì§€: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨")
    finally:
        cap.release()
        print("âœ… ì¹´ë©”ë¼ ìì› í•´ì œ ì™„ë£Œ")

# ==== ë¼ìš°íŒ… ====
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/asl')
def asl_page():
    return render_template('asl.html')

@app.route('/ksl')
def ksl_page():
    return render_template('ksl.html')

@app.route('/video_feed_asl')
def video_feed_asl():
    return Response(generate_frames(asl_interpreter, asl_input_details, asl_output_details, labels_asl, "asl"),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/video_feed_ksl')
def video_feed_ksl():
    return Response(generate_frames(ksl_interpreter, ksl_input_details, ksl_output_details, labels_ksl, "ksl"),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/get_string/<lang>')
def get_string(lang):
    return {'string': recognized_string[lang], 'current': latest_char[lang]}

@app.route('/add_char/<lang>')
def add_char(lang):
    if latest_char[lang] and latest_char[lang] not in ["ERR:IDX", "ERR:DIM"]:
        recognized_string[lang] += latest_char[lang]
    return jsonify({'success': True})

@app.route('/remove_char/<lang>')
def remove_char(lang):
    if recognized_string[lang]:
        recognized_string[lang] = recognized_string[lang][:-1]
    return jsonify({'success': True})

@app.route('/clear_string/<lang>')
def clear_string(lang):
    recognized_string[lang] = ""
    return jsonify({'success': True})

@app.route('/translate/<lang>')
def translate(lang):
    original = combine_hangul_jamo(list(recognized_string[lang].strip())) or "Hello"
    try:
        en = GoogleTranslator(source='auto', target='en').translate(original)
        ko = GoogleTranslator(source='auto', target='ko').translate(original)
        zh = GoogleTranslator(source='auto', target='zh-CN').translate(original)
        ja = GoogleTranslator(source='auto', target='ja').translate(original)
    except Exception as e:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨:", e)
        en = ko = zh = ja = "(ë²ˆì—­ ì˜¤ë¥˜)"

    # ë’¤ë¡œê°€ê¸° ì£¼ì†Œ ê²°ì •
    prev_url = f"/{lang}" if lang in ["asl", "ksl"] else "/"

    return render_template('translate.html', ko=ko, en=en, zh=zh, ja=ja, prev_url=prev_url)


@app.route('/edu/<lang>')
def edu_page(lang):
    string = recognized_string.get(lang, "")
    chars = list(string)
    return render_template("edu.html", chars=chars, lang=lang)

# ==== TTS ìŒì„± ì¶œë ¥ ====
@app.route('/speak/<lang_code>')
def speak(lang_code):
    try:
        # ì¡°í•©ëœ í•œê¸€ ë¬¸ìì—´ ë§Œë“¤ê¸° (ìëª¨ â†’ ì™„ì„±í˜•)
        raw = recognized_string["asl"] or recognized_string["ksl"]
        original_text = combine_hangul_jamo(list(raw.strip())) if raw else ""

        if not original_text:
            return jsonify({'success': False, 'msg': 'ì¸ì‹ëœ ë¬¸ìì—´ì´ ì—†ìŠµë‹ˆë‹¤.'})

        # ë²ˆì—­ ê²°ê³¼ ì‚¬ìš© (ì •í™•í•œ ë°œìŒì„ ìœ„í•´)
        text_map = {
            "ko": original_text,
            "en": GoogleTranslator(source='ko', target='en').translate(original_text),
            "zh": GoogleTranslator(source='ko', target='zh-CN').translate(original_text),
            "ja": GoogleTranslator(source='ko', target='ja').translate(original_text),
        }

        text = text_map.get(lang_code, "")
        if not text:
            return jsonify({'success': False, 'msg': 'í•´ë‹¹ ì–¸ì–´ ì½”ë“œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'})

        tts = gTTS(text=text, lang=lang_code)
        mp3_path = os.path.join(BASE_DIR, "temp.mp3")
        wav_path = os.path.join(BASE_DIR, "temp.wav")
        tts.save(mp3_path)

        subprocess.run(["ffmpeg", "-y", "-i", mp3_path, wav_path],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(["aplay", wav_path],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        return jsonify({'success': True})
    except Exception as e:
        print(f"âŒ ìŒì„± ì¶œë ¥ ì‹¤íŒ¨: {e}")
        return jsonify({'success': False, 'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5002)

